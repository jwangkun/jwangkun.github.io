<!DOCTYPE html>
<html>
  <head>
      <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="yes" name="apple-mobile-web-app-capable" />
  <meta content="black" name="apple-mobile-web-app-status-bar-style" />
  <meta name="referrer" content="never">
  <meta name="keywords" content="">
  <meta name="description" content="">
  <meta name="author" content="kveln">
  <title>在 Kubernetes 上搭建 EFK 日志收集系统 | John Wong&#39;s Blog</title>
  <link href="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/css/bootstrap.min.css" rel="stylesheet">
  <!-- <link href="https://jwangkun.github.io/media/css/bootstrap.min.css" rel="stylesheet"> -->
  <!--  <link href="https://jwangkun.github.io/media/css/all.min.css" rel="stylesheet" type="text/css"> -->
  <link href="https://cdn.bootcss.com/font-awesome/5.11.2/css/all.min.css" rel="stylesheet">
  <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>
  <link rel="alternate" type="application/rss+xml" title="在 Kubernetes 上搭建 EFK 日志收集系统 | John Wong&#39;s Blog » Feed" href="https://jwangkun.github.io/atom.xml">
  <link rel="stylesheet"href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/styles/androidstudio.min.css">
  <link href="https://jwangkun.github.io/styles/main.css" rel="stylesheet">
  <script src="https://cdn.bootcss.com/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.15.10/build/highlight.min.js"></script>
  <!-- <script src="https://jwangkun.github.io/media/scripts/jquery.min.js"></script> -->
  <script>hljs.initHighlightingOnLoad();</script>
  

    <meta property="og:description" content="在 Kubernetes 上搭建 EFK 日志收集系统"/>
    <meta property="og:url" content="https://jwangkun.github.io/tgYRQAxUk/"/>
    <meta property="og:locale" content="zh-CN"/>
    <meta property="og:type" content="website"/>
    <meta property="og:site_name" content="John Wong&#39;s Blog"/>
  </head>
  <body>
  	<!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand" href="https://jwangkun.github.io">John Wong&#39;s Blog</a>
      <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        Menu
        <i class="fas fa-bars"></i>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          
          <li class="nav-item">
              
              <a class="nav-link" href="/">首页</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/archives">归档</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/tags">标签</a>
              
          </li>
          
          <li class="nav-item">
              
              <a class="nav-link" href="/about">关于</a>
              
          </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!-- Page Header -->
  <header class="masthead" style="background-image: url('https://jwangkun.github.io/media/images/home-bg.jpg')">
    <div class="overlay"></div>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <div class="post-heading">
          	<span class="tags">
          	 
            <a href="https://jwangkun.github.io/E5U1mEsrq/" class="tag">Elasticsearch</a>
            
            <a href="https://jwangkun.github.io/nPBE_sUl52/" class="tag">Kibana</a>
            
            <a href="https://jwangkun.github.io/qi6RwSsX4m/" class="tag">EFK</a>
            
            <a href="https://jwangkun.github.io/efVGHoi38/" class="tag">Kubernetes</a>
            
        </span>
            <h1>在 Kubernetes 上搭建 EFK 日志收集系统</h1>
            <span class="meta">
            	Posted on
              2020-10-15，44 min read
            </span>
          </div>
        </div>
      </div>
    </div>
  </header>

  <!-- Post Content -->
  <article>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <p><code>Elasticsearch</code> 是一个实时的、分布式的可扩展的搜索引擎，允许进行全文、结构化搜索，它通常用于索引和搜索大量日志数据，也可用于搜索许多不同类型的文档。</p>
<p>Elasticsearch 通常与 <code>Kibana</code> 一起部署，Kibana 是 Elasticsearch 的一个功能强大的数据可视化 Dashboard，Kibana 允许你通过 web 界面来浏览 Elasticsearch 日志数据。</p>
<!-- more -->
<p><code>Fluentd</code>是一个流行的开源数据收集器，我们将在 Kubernetes 集群节点上安装 Fluentd，通过获取容器日志文件、过滤和转换日志数据，然后将数据传递到 Elasticsearch 集群，在该集群中对其进行索引和存储。</p>
<p>我们先来配置启动一个可扩展的 Elasticsearch 集群，然后在 Kubernetes 集群中创建一个 Kibana 应用，最后通过 DaemonSet 来运行 Fluentd，以便它在每个 Kubernetes 工作节点上都可以运行一个 Pod。</p>
<blockquote>
<p>如果你了解 EFK 的基本原理，只是为了测试可以直接使用 Kubernetes 官方提供的 addon 插件的资源清单，地址：https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/，直接安装即可。</p>
</blockquote>
<p>本文在之前环境的基础上升级到最新版本，实验环境版本：</p>
<ul>
<li>Kubernetes：<code>v1.16.2</code></li>
<li>Elasticsearch 镜像：<code>docker.elastic.co/elasticsearch/elasticsearch:7.6.2</code></li>
<li>Kibana 镜像：<code>docker.elastic.co/kibana/kibana:7.6.2</code></li>
<li>Fluentd 镜像：<code>quay.io/fluentd_elasticsearch/fluentd:v3.0.1</code></li>
<li>elastalert 镜像：<code>jertel/elastalert-docker:0.2.4</code></li>
<li>Rook Ceph 镜像：<code>rook/ceph:v1.2.1</code></li>
</ul>
<h2 id="创建-elasticsearch-集群">创建 Elasticsearch 集群</h2>
<p>在创建 Elasticsearch 集群之前，我们先创建一个命名空间，我们将在其中安装所有日志相关的资源对象。</p>
<p>新建一个 kube-logging.yaml 文件：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Namespace
metadata:
  name: logging
</code></pre>
<p>然后通过 kubectl 创建该资源清单，创建一个名为 logging 的 namespace：</p>
<pre><code class="language-shell">$ kubectl create -f kube-logging.yaml
namespace/logging created
$ kubectl get ns
NAME           STATUS    AGE
default        Active    244d
istio-system   Active    100d
kube-ops       Active    179d
kube-public    Active    244d
kube-system    Active    244d
logging        Active    4h
monitoring     Active    35d
</code></pre>
<p>现在创建了一个命名空间来存放我们的日志相关资源，接下来可以部署 EFK 相关组件，首先开始部署一个3节点的 Elasticsearch 集群。</p>
<p>这里我们使用3个 Elasticsearch Pod 来避免高可用下多节点集群中出现的“脑裂”问题，当一个或多个节点无法与其他节点通信时会产生“脑裂”，可能会出现几个主节点。</p>
<blockquote>
<p>了解更多 Elasticsearch 集群脑裂问题，可以查看文档https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain</p>
</blockquote>
<p>一个关键点是您应该设置参数<code>discover.zen.minimum_master_nodes=N/2+1</code>，其中<code>N</code>是 Elasticsearch 集群中符合主节点的节点数，比如我们这里3个节点，意味着<code>N</code>应该设置为2。这样，如果一个节点暂时与集群断开连接，则另外两个节点可以选择一个新的主节点，并且集群可以在最后一个节点尝试重新加入时继续运行，在扩展 Elasticsearch 集群时，一定要记住这个参数。首先创建一个名为 elasticsearch 的无头服务，新建文件 elasticsearch-svc.yaml，文件内容如下：</p>
<pre><code class="language-yaml">kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node
</code></pre>
<p>定义了一个名为 elasticsearch 的 Service，指定标签 <code>app=elasticsearch</code>，当我们将 Elasticsearch StatefulSet 与此服务关联时，服务将返回带有标签 <code>app=elasticsearch</code>的 Elasticsearch Pods 的 DNS A 记录，然后设置 <code>clusterIP=None</code>，将该服务设置成无头服务。最后，我们分别定义端口9200、9300，分别用于与 REST API 交互，以及用于节点间通信。</p>
<p>使用 kubectl 直接创建上面的服务资源对象：</p>
<pre><code class="language-shell">$ kubectl create -f elasticsearch-svc.yaml
service/elasticsearch created
$ kubectl get services --namespace=logging
Output
NAME            TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
elasticsearch   ClusterIP   None         &lt;none&gt;        9200/TCP,9300/TCP   26s
</code></pre>
<p>现在我们已经为 Pod 设置了无头服务和一个稳定的域名<code>.elasticsearch.logging.svc.cluster.local</code>，接下来我们通过 StatefulSet 来创建具体的 Elasticsearch 的 Pod 应用。</p>
<p>Kubernetes StatefulSet 允许我们为 Pod 分配一个稳定的标识和持久化存储，Elasticsearch 需要稳定的存储来保证 Pod 在重新调度或者重启后的数据依然不变，所以需要使用 StatefulSet 来管理 Pod。</p>
<blockquote>
<p>要了解更多关于 StaefulSet 的信息，可以查看官网关于 StatefulSet 的相关文档：https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/。</p>
</blockquote>
<p>新建名为 elasticsearch-statefulset.yaml 的资源清单文件，首先粘贴下面内容：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
</code></pre>
<p>该内容中，我们定义了一个名为 es 的 StatefulSet 对象，然后定义<code>serviceName=elasticsearch</code>和前面创建的 Service 相关联，这可以确保使用以下 DNS 地址访问 StatefulSet 中的每一个 Pod：<code>es-[0,1,2].elasticsearch.logging.svc.cluster.local</code>，其中[0,1,2]对应于已分配的 Pod 序号。</p>
<p>然后指定3个副本，将 matchLabels 设置为<code>app=elasticsearch</code>，所以 Pod 的模板部分<code>.spec.template.metadata.lables</code>也必须包含<code>app=elasticsearch</code>标签。</p>
<p>然后定义 Pod 模板部分内容：</p>
<pre><code class="language-yaml">...
  spec:
    containers:
    - name: elasticsearch
      image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2
      resources:
        limits:
          cpu: 1000m
        requests:
          cpu: 100m
      ports:
      - containerPort: 9200
        name: rest
        protocol: TCP
      - containerPort: 9300
        name: inter-node
        protocol: TCP
      volumeMounts:
      - name: data
        mountPath: /usr/share/elasticsearch/data
      env:
        - name: cluster.name
          value: k8s-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: cluster.initial_master_nodes
          value: &quot;es-0,es-1,es-2&quot;
        - name: discovery.zen.minimum_master_nodes
          value: &quot;2&quot;
        - name: discovery.seed_hosts
          value: &quot;elasticsearch&quot;
        - name: ES_JAVA_OPTS
          value: &quot;-Xms512m -Xmx512m&quot;
        - name: network.host
          value: &quot;0.0.0.0&quot;
</code></pre>
<p>该部分是定义 StatefulSet 中的 Pod，暴露了9200和9300两个端口，注意名称要和上面定义的 Service 保持一致。然后通过 volumeMount 声明了数据持久化目录，下面我们再来定义 VolumeClaims。最后就是我们在容器中设置的一些环境变量了：</p>
<ul>
<li>cluster.name：Elasticsearch 集群的名称，我们这里命名成 k8s-logs。</li>
<li>node.name：节点的名称，通过 <code>metadata.name</code> 来获取。这将解析为 es-[0,1,2]，取决于节点的指定顺序。</li>
<li>discovery.seed_hosts：此字段用于设置在 Elasticsearch 集群中节点相互连接的发现方法。由于我们之前配置的无头服务，我们的 Pod 具有唯一的 DNS 域<code>es-[0,1,2].elasticsearch.logging.svc.cluster.local</code>，因此我们相应地设置此变量。要了解有关 Elasticsearch 发现的更多信息，请参阅 Elasticsearch 官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html。</li>
<li>discovery.zen.minimum_master_nodes：我们将其设置为<code>(N/2) + 1</code>，<code>N</code>是我们的群集中符合主节点的节点的数量。我们有3个 Elasticsearch 节点，因此我们将此值设置为2（向下舍入到最接近的整数）。要了解有关此参数的更多信息，请参阅官方 Elasticsearch 文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html#split-brain。</li>
<li>ES_JAVA_OPTS：这里我们设置为<code>-Xms512m -Xmx512m</code>，告诉<code>JVM</code>使用<code>512 MB</code>的最小和最大堆。您应该根据群集的资源可用性和需求调整这些参数。要了解更多信息，请参阅设置堆大小的相关文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/heap-size.html。</li>
</ul>
<p>接下来添加关于 initContainer 的内容：</p>
<pre><code class="language-yaml">...
    initContainers:
    - name: increase-vm-max-map
      image: busybox
      command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;]
      securityContext:
        privileged: true
    - name: increase-fd-ulimit
      image: busybox
      command: [&quot;sh&quot;, &quot;-c&quot;, &quot;ulimit -n 65536&quot;]
      securityContext:
        privileged: true
</code></pre>
<p>这里我们定义了几个在主应用程序之前运行的 Init 容器，这些初始容器按照定义的顺序依次执行，执行完成后才会启动主应用容器。</p>
<p>第一个名为 increase-vm-max-map 的容器用来增加操作系统对<code>mmap</code>计数的限制，默认情况下该值可能太低，导致内存不足的错误，要了解更多关于该设置的信息，可以查看 Elasticsearch 官方文档说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html。</p>
<p>最后一个初始化容器是用来执行<code>ulimit</code>命令增加打开文件描述符的最大数量的。</p>
<blockquote>
<p>此外 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_notes_for_production_use_and_defaults">Elastisearch Notes for Production Use</a> 文档还提到了由于性能原因最好禁用 swap，当然对于 Kubernetes 集群而言，最好也是禁用 swap 分区的。</p>
</blockquote>
<p>现在我们已经定义了主应用容器和它之前运行的 Init Containers 来调整一些必要的系统参数，接下来我们可以添加数据目录的持久化相关的配置，在 StatefulSet 中，使用 volumeClaimTemplates 来定义 volume 模板即可：</p>
<pre><code class="language-yaml">...
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: rook-ceph-block
      resources:
        requests:
          storage: 50Gi
</code></pre>
<p>我们这里使用 volumeClaimTemplates 来定义持久化模板，Kubernetes 会使用它为 Pod 创建 PersistentVolume，设置访问模式为<code>ReadWriteOnce</code>，这意味着它只能被 mount 到单个节点上进行读写，然后最重要的是使用了一个 StorageClass 对象，这里我们就直接使用前面创建的 Ceph RBD 类型的名为 <code>rook-ceph-block</code> 的 StorageClass 对象即可。最后，我们指定了每个 PersistentVolume 的大小为 50GB，我们可以根据自己的实际需要进行调整该值。</p>
<p>完整的 Elasticsearch StatefulSet 资源清单文件内容如下：</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels: 
        app: elasticsearch
    spec:
      nodeSelector:
        es: log
      initContainers:
      - name: increase-vm-max-map
        image: busybox
        command: [&quot;sysctl&quot;, &quot;-w&quot;, &quot;vm.max_map_count=262144&quot;]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        command: [&quot;sh&quot;, &quot;-c&quot;, &quot;ulimit -n 65536&quot;]
        securityContext:
          privileged: true
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2
        ports:
        - name: rest
          containerPort: 9200
        - name: inter
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 1000m
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
        - name: cluster.name
          value: k8s-logs
        - name: node.name
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: cluster.initial_master_nodes
          value: &quot;es-0,es-1,es-2&quot;
        - name: discovery.zen.minimum_master_nodes
          value: &quot;2&quot;
        - name: discovery.seed_hosts
          value: &quot;elasticsearch&quot;
        - name: ES_JAVA_OPTS
          value: &quot;-Xms512m -Xmx512m&quot;
        - name: network.host
          value: &quot;0.0.0.0&quot;
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ &quot;ReadWriteOnce&quot; ]
      storageClassName: rook-ceph-block
      resources:
        requests:
          storage: 50Gi    
</code></pre>
<p>现在直接使用 kubectl 工具部署即可：</p>
<pre><code class="language-shell">$ kubectl create -f elasticsearch-statefulset.yaml
statefulset.apps/es created
</code></pre>
<p>添加成功后，可以看到 logging 命名空间下面的所有的资源对象：</p>
<pre><code class="language-shell">$ kubectl get sts -n logging
NAME   READY   AGE
es     3/3     83m
$ kubectl get pods -n logging
NAME                      READY   STATUS    RESTARTS   AGE
es-0                      1/1     Running   0          83m
es-1                      1/1     Running   0          82m
es-2                      1/1     Running   0          81m
$ kubectl get svc -n logging
NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
elasticsearch   ClusterIP   None             &lt;none&gt;        9200/TCP,9300/TCP   20h
</code></pre>
<p>Pods 部署完成后，我们可以通过请求一个 REST API 来检查 Elasticsearch 集群是否正常运行。使用下面的命令将本地端口9200 转发到 Elasticsearch 节点（如es-0）对应的端口：</p>
<pre><code class="language-shell">$ kubectl port-forward es-0 9200:9200 --namespace=logging
Forwarding from 127.0.0.1:9200 -&gt; 9200
Forwarding from [::1]:9200 -&gt; 9200
</code></pre>
<p>然后，在另外的终端窗口中，执行如下请求：</p>
<pre><code class="language-shell">$ curl http://localhost:9200/_cluster/state?pretty
</code></pre>
<p>正常来说，应该会看到类似于如下的信息：</p>
<pre><code class="language-shell">{
  &quot;cluster_name&quot; : &quot;k8s-logs&quot;,
  &quot;compressed_size_in_bytes&quot; : 348,
  &quot;cluster_uuid&quot; : &quot;QD06dK7CQgids-GQZooNVw&quot;,
  &quot;version&quot; : 3,
  &quot;state_uuid&quot; : &quot;mjNIWXAzQVuxNNOQ7xR-qg&quot;,
  &quot;master_node&quot; : &quot;IdM5B7cUQWqFgIHXBp0JDg&quot;,
  &quot;blocks&quot; : { },
  &quot;nodes&quot; : {
    &quot;u7DoTpMmSCixOoictzHItA&quot; : {
      &quot;name&quot; : &quot;es-1&quot;,
      &quot;ephemeral_id&quot; : &quot;ZlBflnXKRMC4RvEACHIVdg&quot;,
      &quot;transport_address&quot; : &quot;10.244.4.191:9300&quot;,
      &quot;attributes&quot; : { }
    },
    &quot;IdM5B7cUQWqFgIHXBp0JDg&quot; : {
      &quot;name&quot; : &quot;es-0&quot;,
      &quot;ephemeral_id&quot; : &quot;JTk1FDdFQuWbSFAtBxdxAQ&quot;,
      &quot;transport_address&quot; : &quot;10.244.2.215:9300&quot;,
      &quot;attributes&quot; : { }
    },
    &quot;R8E7xcSUSbGbgrhAdyAKmQ&quot; : {
      &quot;name&quot; : &quot;es-2&quot;,
      &quot;ephemeral_id&quot; : &quot;9wv6ke71Qqy9vk2LgJTqaA&quot;,
      &quot;transport_address&quot; : &quot;10.244.40.4:9300&quot;,
      &quot;attributes&quot; : { }
    }
  },
...
</code></pre>
<p>看到上面的信息就表明我们名为 k8s-logs 的 Elasticsearch 集群成功创建了3个节点：es-0，es-1，和es-2，当前主节点是 es-0。</p>
<h2 id="创建-kibana-服务">创建 Kibana 服务</h2>
<p>Elasticsearch 集群启动成功了，接下来我们可以来部署 Kibana 服务，新建一个名为 kibana.yaml 的文件，对应的文件内容如下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  ports:
  - port: 5601
  type: NodePort
  selector:
    app: kibana

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      nodeSelector:
        es: log
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.6.2
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 1000m
        env:
        - name: ELASTICSEARCH_HOSTS
          value: http://elasticsearch:9200
        ports:
        - containerPort: 5601
</code></pre>
<p>上面我们定义了两个资源对象，一个 Service 和 Deployment，为了测试方便，我们将 Service 设置为了 NodePort 类型，Kibana Pod 中配置都比较简单，唯一需要注意的是我们使用 <code>ELASTICSEARCH_HOSTS</code> 这个环境变量来设置Elasticsearch 集群的端点和端口，直接使用 Kubernetes DNS 即可，此端点对应服务名称为 elasticsearch，由于是一个 headless service，所以该域将解析为3个 Elasticsearch Pod 的 IP 地址列表。配置完成后，直接使用 kubectl 工具创建：</p>
<pre><code class="language-shell">$ kubectl create -f kibana.yaml
service/kibana created
deployment.apps/kibana created
</code></pre>
<p>创建完成后，可以查看 Kibana Pod 的运行状态：</p>
<pre><code class="language-shell">$ kubectl get pods --namespace=logging
NAME                      READY   STATUS    RESTARTS   AGE
es-0                      1/1     Running   0          85m
es-1                      1/1     Running   0          84m
es-2                      1/1     Running   0          83m
kibana-5c565c47dd-xj4bd   1/1     Running   0          80m
$ kubectl get svc -n logging
NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
elasticsearch   ClusterIP   None            &lt;none&gt;        9200/TCP,9300/TCP   3h22m
kibana          NodePort    10.111.223.99   &lt;none&gt;        5601:31139/TCP      3h20m
</code></pre>
<p>如果 Pod 已经是 Running 状态了，证明应用已经部署成功了，然后可以通过 NodePort 来访问 Kibana 这个服务，在浏览器中打开<code>http://&lt;任意节点IP&gt;:31139</code>即可，如果看到如下欢迎界面证明 Kibana 已经成功部署到了 Kubernetes集群之中。</p>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200427174820.png" alt="kibana welcome" loading="lazy">kibana welcome</p>
<h2 id="部署-fluentd">部署 Fluentd</h2>
<p><code>Fluentd</code> 是一个高效的日志聚合器，是用 Ruby 编写的，并且可以很好地扩展。对于大部分企业来说，Fluentd 足够高效并且消耗的资源相对较少，另外一个工具<code>Fluent-bit</code>更轻量级，占用资源更少，但是插件相对 Fluentd 来说不够丰富，所以整体来说，Fluentd 更加成熟，使用更加广泛，所以我们这里也同样使用 Fluentd 来作为日志收集工具。</p>
<h3 id="工作原理">工作原理</h3>
<p>Fluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下：</p>
<ul>
<li>首先 Fluentd 从多个日志源获取数据</li>
<li>结构化并且标记这些数据</li>
<li>然后根据匹配的标签将数据发送到多个目标服务去</li>
</ul>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/7moPNc.jpg" alt="fluentd 架构" loading="lazy">fluentd 架构</p>
<h3 id="配置">配置</h3>
<p>一般来说我们是通过一个配置文件来告诉 Fluentd 如何采集、处理数据的，下面简单和大家介绍下 Fluentd 的配置方法。</p>
<h4 id="日志源配置">日志源配置</h4>
<p>比如我们这里为了收集 Kubernetes 节点上的所有容器日志，就需要做如下的日志源配置：</p>
<pre><code>&lt;source&gt;
  @id fluentd-containers.log
  @type tail                             # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。
  path /var/log/containers/*.log         # 挂载的服务器Docker容器日志地址
  pos_file /var/log/es-containers.log.pos
  tag raw.kubernetes.*                   # 设置日志标签
  read_from_head true
  &lt;parse&gt;                                # 多行格式化成JSON
    @type multi_format                   # 使用 multi-format-parser 解析器插件
    &lt;pattern&gt;
      format json                        # JSON 解析器
      time_key time                      # 指定事件时间的时间字段
      time_format %Y-%m-%dT%H:%M:%S.%NZ  # 时间格式
    &lt;/pattern&gt;
    &lt;pattern&gt;
      format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/
      time_format %Y-%m-%dT%H:%M:%S.%N%:z
    &lt;/pattern&gt;
  &lt;/parse&gt;
&lt;/source&gt;
</code></pre>
<p>上面配置部分参数说明如下：</p>
<ul>
<li>id：表示引用该日志源的唯一标识符，该标识可用于进一步过滤和路由结构化日志数据</li>
<li>type：Fluentd 内置的指令，<code>tail</code> 表示 Fluentd 从上次读取的位置通过 tail 不断获取数据，另外一个是 <code>http</code> 表示通过一个 GET 请求来收集数据。</li>
<li>path：<code>tail</code> 类型下的特定参数，告诉 Fluentd 采集 <code>/var/log/containers</code> 目录下的所有日志，这是 docker 在 Kubernetes 节点上用来存储运行容器 stdout 输出日志数据的目录。</li>
<li>pos_file：检查点，如果 Fluentd 程序重新启动了，它将使用此文件中的位置来恢复日志数据收集。</li>
<li>tag：用来将日志源与目标或者过滤器匹配的自定义字符串，Fluentd 匹配源/目标标签来路由日志数据。</li>
</ul>
<h4 id="路由配置">路由配置</h4>
<p>上面是日志源的配置，接下来看看如何将日志数据发送到 Elasticsearch：</p>
<pre><code>&lt;match **&gt;

@id elasticsearch

@type elasticsearch

@log_level info

include_tag_key true

type_name fluentd

host &quot;#{ENV['OUTPUT_HOST']}&quot;

port &quot;#{ENV['OUTPUT_PORT']}&quot;

logstash_format true

&lt;buffer&gt;

@type file

path /var/log/fluentd-buffers/kubernetes.system.buffer

flush_mode interval

retry_type exponential_backoff

flush_thread_count 2

flush_interval 5s

retry_forever

retry_max_interval 30

chunk_limit_size &quot;#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}&quot;

queue_limit_length &quot;#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}&quot;

overflow_action block

&lt;/buffer&gt;
</code></pre>
<ul>
<li>match：标识一个目标标签，后面是一个匹配日志源的正则表达式，我们这里想要捕获所有的日志并将它们发送给 Elasticsearch，所以需要配置成<code>**</code>。</li>
<li>id：目标的一个唯一标识符。</li>
<li>type：支持的输出插件标识符，我们这里要输出到 Elasticsearch，所以配置成 elasticsearch，这是 Fluentd 的一个内置插件。</li>
<li>log_level：指定要捕获的日志级别，我们这里配置成 <code>info</code>，表示任何该级别或者该级别以上（INFO、WARNING、ERROR）的日志都将被路由到 Elsasticsearch。</li>
<li>host/port：定义 Elasticsearch 的地址，也可以配置认证信息，我们的 Elasticsearch 不需要认证，所以这里直接指定 host 和 port 即可。</li>
<li>logstash_format：Elasticsearch 服务对日志数据构建反向索引进行搜索，将 logstash_format 设置为 <code>true</code>，Fluentd 将会以 logstash 格式来转发结构化的日志数据。</li>
<li>Buffer： Fluentd 允许在目标不可用时进行缓存，比如，如果网络出现故障或者 Elasticsearch 不可用的时候。缓冲区配置也有助于降低磁盘的 IO。</li>
</ul>
<h4 id="过滤">过滤</h4>
<p>由于 Kubernetes 集群中应用太多，也还有很多历史数据，所以我们可以只将某些应用的日志进行收集，比如我们只采集具有 <code>logging=true</code> 这个 Label 标签的 Pod 日志，这个时候就需要使用 filter，如下所示：</p>
<pre><code class="language-yaml"># 删除无用的属性
&lt;filter kubernetes.**&gt;
  @type record_transformer
  remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash
&lt;/filter&gt;
# 只保留具有logging=true标签的Pod日志
&lt;filter kubernetes.**&gt;
  @id filter_log
  @type grep
  &lt;regexp&gt;
    key $.kubernetes.labels.logging
    pattern ^true$
  &lt;/regexp&gt;
&lt;/filter&gt;
</code></pre>
<h3 id="安装">安装</h3>
<p>要收集 Kubernetes 集群的日志，直接用 DasemonSet 控制器来部署 Fluentd 应用，这样，它就可以从 Kubernetes 节点上采集日志，确保在集群中的每个节点上始终运行一个 Fluentd 容器。当然可以直接使用 Helm 来进行一键安装，为了能够了解更多实现细节，我们这里还是采用手动方法来进行安装。</p>
<p>首先，我们通过 ConfigMap 对象来指定 Fluentd 配置文件，新建 fluentd-configmap.yaml 文件，文件内容如下：</p>
<pre><code class="language-yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: fluentd-config
  namespace: logging
data:
  system.conf: |-
    &lt;system&gt;
      root_dir /tmp/fluentd-buffers/
    &lt;/system&gt;
  containers.input.conf: |-
    &lt;source&gt;
      @id fluentd-containers.log
      @type tail                              # Fluentd 内置的输入方式，其原理是不停地从源文件中获取新的日志。
      path /var/log/containers/*.log          # 挂载的服务器Docker容器日志地址
      pos_file /var/log/es-containers.log.pos
      tag raw.kubernetes.*                    # 设置日志标签
      read_from_head true
      &lt;parse&gt;                                 # 多行格式化成JSON
        @type multi_format                    # 使用 multi-format-parser 解析器插件
        &lt;pattern&gt;
          format json                         # JSON解析器
          time_key time                       # 指定事件时间的时间字段
          time_format %Y-%m-%dT%H:%M:%S.%NZ   # 时间格式
        &lt;/pattern&gt;
        &lt;pattern&gt;
          format /^(?&lt;time&gt;.+) (?&lt;stream&gt;stdout|stderr) [^ ]* (?&lt;log&gt;.*)$/
          time_format %Y-%m-%dT%H:%M:%S.%N%:z
        &lt;/pattern&gt;
      &lt;/parse&gt;
    &lt;/source&gt;
    # 在日志输出中检测异常，并将其作为一条日志转发 
    # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions
    &lt;match raw.kubernetes.**&gt;           # 匹配tag为raw.kubernetes.**日志信息
      @id raw.kubernetes
      @type detect_exceptions           # 使用detect-exceptions插件处理异常栈信息
      remove_tag_prefix raw             # 移除 raw 前缀
      message log                       
      stream stream                     
      multiline_flush_interval 5
      max_bytes 500000
      max_lines 1000
    &lt;/match&gt;

    &lt;filter **&gt;  # 拼接日志
      @id filter_concat
      @type concat                # Fluentd Filter 插件，用于连接多个事件中分隔的多行日志。
      key message
      multiline_end_regexp /\n$/  # 以换行符“\n”拼接
      separator &quot;&quot;
    &lt;/filter&gt; 

    # 添加 Kubernetes metadata 数据
    &lt;filter kubernetes.**&gt;
      @id filter_kubernetes_metadata
      @type kubernetes_metadata
    &lt;/filter&gt;

    # 修复 ES 中的 JSON 字段
    # 插件地址：https://github.com/repeatedly/fluent-plugin-multi-format-parser
    &lt;filter kubernetes.**&gt;
      @id filter_parser
      @type parser                # multi-format-parser多格式解析器插件
      key_name log                # 在要解析的记录中指定字段名称。
      reserve_data true           # 在解析结果中保留原始键值对。
      remove_key_name_field true  # key_name 解析成功后删除字段。
      &lt;parse&gt;
        @type multi_format
        &lt;pattern&gt;
          format json
        &lt;/pattern&gt;
        &lt;pattern&gt;
          format none
        &lt;/pattern&gt;
      &lt;/parse&gt;
    &lt;/filter&gt;

    # 删除一些多余的属性
    &lt;filter kubernetes.**&gt;
      @type record_transformer
      remove_keys $.docker.container_id,$.kubernetes.container_image_id,$.kubernetes.pod_id,$.kubernetes.namespace_id,$.kubernetes.master_url,$.kubernetes.labels.pod-template-hash
    &lt;/filter&gt;

    # 只保留具有logging=true标签的Pod日志
    &lt;filter kubernetes.**&gt;
      @id filter_log
      @type grep
      &lt;regexp&gt;
        key $.kubernetes.labels.logging
        pattern ^true$
      &lt;/regexp&gt;
    &lt;/filter&gt;
  
  ###### 监听配置，一般用于日志聚合用 ######
  forward.input.conf: |-
    # 监听通过TCP发送的消息
    &lt;source&gt;
      @id forward
      @type forward
    &lt;/source&gt;

  output.conf: |-
    &lt;match **&gt;
      @id elasticsearch
      @type elasticsearch
      @log_level info
      include_tag_key true
      host elasticsearch
      port 9200
      logstash_format true
      logstash_prefix k8s  # 设置 index 前缀为 k8s
      request_timeout    30s
      &lt;buffer&gt;
        @type file
        path /var/log/fluentd-buffers/kubernetes.system.buffer
        flush_mode interval
        retry_type exponential_backoff
        flush_thread_count 2
        flush_interval 5s
        retry_forever
        retry_max_interval 30
        chunk_limit_size 2M
        queue_limit_length 8
        overflow_action block
      &lt;/buffer&gt;
    &lt;/match&gt;
</code></pre>
<p>上面配置文件中我们只配置了 docker 容器日志目录，收集到数据经过处理后发送到 <code>elasticsearch:9200</code> 服务。</p>
<p>然后新建一个 fluentd-daemonset.yaml 的文件，文件内容如下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd-es
  namespace: logging
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - &quot;namespaces&quot;
  - &quot;pods&quot;
  verbs:
  - &quot;get&quot;
  - &quot;watch&quot;
  - &quot;list&quot;
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: fluentd-es
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: fluentd-es
  namespace: logging
  apiGroup: &quot;&quot;
roleRef:
  kind: ClusterRole
  name: fluentd-es
  apiGroup: &quot;&quot;
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-es
  namespace: logging
  labels:
    k8s-app: fluentd-es
    kubernetes.io/cluster-service: &quot;true&quot;
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  selector:
    matchLabels:
      k8s-app: fluentd-es
  template:
    metadata:
      labels:
        k8s-app: fluentd-es
        kubernetes.io/cluster-service: &quot;true&quot;
      # 此注释确保如果节点被驱逐，fluentd不会被驱逐，支持关键的基于 pod 注释的优先级方案。
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      serviceAccountName: fluentd-es
      containers:
      - name: fluentd-es
        image: quay.io/fluentd_elasticsearch/fluentd:v3.0.1
        env:
        - name: FLUENTD_ARGS
          value: --no-supervisor -q
        resources:
          limits:
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /data/docker/containers
          readOnly: true
        - name: config-volume
          mountPath: /etc/fluent/config.d
      nodeSelector:
        beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;
      tolerations:
      - operator: Exists
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /data/docker/containers
      - name: config-volume
        configMap:
          name: fluentd-config
</code></pre>
<p>我们将上面创建的 fluentd-config 这个 ConfigMap 对象通过 volumes 挂载到了 Fluentd 容器中，另外为了能够灵活控制哪些节点的日志可以被收集，所以我们这里还添加了一个 nodSelector 属性：</p>
<pre><code class="language-yaml">nodeSelector:
  beta.kubernetes.io/fluentd-ds-ready: &quot;true&quot;
</code></pre>
<p>意思就是要想采集节点的日志，那么我们就需要给节点打上上面的标签，比如我们这里只给节点4和节点6打上了该标签：</p>
<pre><code class="language-shell">$ kubectl get nodes --show-labels
NAME          STATUS   ROLES    AGE    VERSION   LABELS
ydzs-master   Ready    master   170d   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=
ydzs-node1    Ready    &lt;none&gt;   170d   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=log,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node1,kubernetes.io/os=linux
ydzs-node2    Ready    &lt;none&gt;   170d   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,com=youdianzhishi,es=log,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node2,kubernetes.io/os=linux
ydzs-node3    Ready    &lt;none&gt;   169d   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,es=log,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node3,kubernetes.io/os=linux,monitor=prometheus
ydzs-node4    Ready    &lt;none&gt;   169d   v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/fluentd-ds-ready=true,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node4,kubernetes.io/os=linux
ydzs-node5    Ready    &lt;none&gt;   96d    v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node5,kubernetes.io/os=linux
ydzs-node6    Ready    &lt;none&gt;   96d    v1.16.2   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/fluentd-ds-ready=true,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=ydzs-node6,kubernetes.io/os=linux
</code></pre>
<blockquote>
<p>如果你需要在其他节点上采集日志，则需要给对应节点打上标签，使用如下命令：<code>kubectl label nodes node名 beta.kubernetes.io/fluentd-ds-ready=true</code>。</p>
</blockquote>
<p>另外由于我们的集群使用的是 kubeadm 搭建的，默认情况下 master 节点有污点，所以如果要想也收集 master 节点的日志，则需要添加上容忍：</p>
<pre><code class="language-yaml">tolerations:
- operator: Exists
</code></pre>
<p>另外需要注意的地方是，我这里的测试环境更改了 docker 的根目录：</p>
<pre><code class="language-shell">$ docker info
...
Docker Root Dir: /data/docker
...
</code></pre>
<p>所以上面要获取 docker 的容器目录需要更改成<code>/data/docker/containers</code>，这个地方非常重要，当然如果你没有更改 docker 根目录则使用默认的<code>/var/lib/docker/containers</code>目录即可。</p>
<p>分别创建上面的 ConfigMap 对象和 DaemonSet：</p>
<pre><code class="language-shell">$ kubectl create -f fluentd-configmap.yaml
configmap &quot;fluentd-config&quot; created
$ kubectl create -f fluentd-daemonset.yaml
serviceaccount &quot;fluentd-es&quot; created
clusterrole.rbac.authorization.k8s.io &quot;fluentd-es&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;fluentd-es&quot; created
daemonset.apps &quot;fluentd-es&quot; created
</code></pre>
<p>创建完成后，查看对应的 Pods 列表，检查是否部署成功：</p>
<pre><code class="language-shell">$ kubectl get pods -n logging
NAME                      READY   STATUS    RESTARTS   AGE
es-0                      1/1     Running   0          108m
es-1                      1/1     Running   0          107m
es-2                      1/1     Running   0          106m
fluentd-es-h4jl2          1/1     Running   0          100m
fluentd-es-vngmd          1/1     Running   0          100m
kibana-5c565c47dd-xj4bd   1/1     Running   0          103m
</code></pre>
<p>Fluentd 启动成功后，这个时候就可以发送日志到 ES 了，但是我们这里是过滤了只采集具有 <code>logging=true</code> 标签的 Pod 日志，所以现在还没有任何数据会被采集。</p>
<p>下面我们部署一个简单的测试应用， 新建 counter.yaml 文件，文件内容如下：</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
  name: counter
  labels:
    logging: &quot;true&quot;  # 一定要具有该标签才会被采集
spec:
  containers:
  - name: count
    image: busybox
    args: [/bin/sh, -c,
            'i=0; while true; do echo &quot;$i: $(date)&quot;; i=$((i+1)); sleep 1; done']
</code></pre>
<p>该 Pod 只是简单将日志信息打印到 <code>stdout</code>，所以正常来说 Fluentd 会收集到这个日志数据，在 Kibana 中也就可以找到对应的日志数据了，使用 kubectl 工具创建该 Pod：</p>
<pre><code class="language-shell">$ kubectl create -f counter.yaml
$ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
counter                          1/1     Running   0          9h
</code></pre>
<p>Pod 创建并运行后，回到 Kibana Dashboard 页面，点击左侧最下面的 <code>management</code> 图标，然后点击 Kibana 下面的 <code>Index Patterns</code> 开始导入索引数据：</p>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200427175019.png" alt="create index" loading="lazy">create index</p>
<p>在这里可以配置我们需要的 Elasticsearch 索引，前面 Fluentd 配置文件中我们采集的日志使用的是 logstash 格式，定义了一个 <code>k8s</code> 的前缀，所以这里只需要在文本框中输入<code>k8s-*</code>即可匹配到 Elasticsearch 集群中采集的 Kubernetes 集群日志数据，然后点击下一步，进入以下页面：</p>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200427175255.png" alt="index config" loading="lazy">index config</p>
<p>在该页面中配置使用哪个字段按时间过滤日志数据，在下拉列表中，选择<code>@timestamp</code>字段，然后点击<code>Create index pattern</code>，创建完成后，点击左侧导航菜单中的<code>Discover</code>，然后就可以看到一些直方图和最近采集到的日志数据了：</p>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200427175432.png" alt="log data" loading="lazy">log data</p>
<p>现在的数据就是上面 Counter 应用的日志，如果还有其他的应用，我们也可以筛选过滤：</p>
<p><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200427193356.png" alt="counter log data" loading="lazy">counter log data</p>
<p>我们也可以通过其他元数据来过滤日志数据，比如您可以单击任何日志条目以查看其他元数据，如容器名称，Kubernetes 节点，命名空间等。</p>
<h2 id="日志分析">日志分析</h2>
<p>上面我们已经可以将应用日志收集起来了，下面我们来使用一个应用演示如何分析采集的日志。示例应用会输出如下所示的 JSON 格式的日志信息：</p>
<pre><code class="language-json">{&quot;LOGLEVEL&quot;:&quot;WARNING&quot;,&quot;serviceName&quot;:&quot;msg-processor&quot;,&quot;serviceEnvironment&quot;:&quot;staging&quot;,&quot;message&quot;:&quot;WARNING client connection terminated unexpectedly.&quot;}
{&quot;LOGLEVEL&quot;:&quot;INFO&quot;,&quot;serviceName&quot;:&quot;msg-processor&quot;,&quot;serviceEnvironment&quot;:&quot;staging&quot;,&quot;message&quot;:&quot;&quot;,&quot;eventsNumber&quot;:5}
{&quot;LOGLEVEL&quot;:&quot;INFO&quot;,&quot;serviceName&quot;:&quot;msg-receiver-api&quot;:&quot;msg-receiver-api&quot;,&quot;serviceEnvironment&quot;:&quot;staging&quot;,&quot;volume&quot;:14,&quot;message&quot;:&quot;API received messages&quot;}
{&quot;LOGLEVEL&quot;:&quot;ERROR&quot;,&quot;serviceName&quot;:&quot;msg-receiver-api&quot;,&quot;serviceEnvironment&quot;:&quot;staging&quot;,&quot;message&quot;:&quot;ERROR Unable to upload files for processing&quot;}
</code></pre>
<p>因为 JSON 格式的日志解析非常容易，当我们将日志结构化传输到 ES 过后，我们可以根据特定的字段值而不是文本搜索日志数据，当然纯文本格式的日志我们也可以进行结构化，但是这样每个应用的日志格式不统一，都需要单独进行结构化，非常麻烦，所以建议将日志格式统一成 JSON 格式输出。</p>
<p>我们这里的示例应用会定期输出不同类型的日志消息，包含不同日志级别（INFO/WARN/ERROR）的日志，一行 JSON 日志就是我们收集的一条日志消息，该消息通过 fluentd 进行采集发送到 Elasticsearch。这里我们会使用到 fluentd 里面的自动 JSON 解析插件，默认情况下，fluentd 会将每个日志文件的一行作为名为 <code>log</code> 的字段进行发送，并自动添加其他字段，比如 <code>tag</code> 标识容器，<code>stream</code> 标识 stdout 或者 stderr。</p>
<p>由于在 fluentd 配置中我们添加了如下所示的过滤器：</p>
<pre><code class="language-yaml">&lt;filter kubernetes.**&gt;
  @id filter_parser
  @type parser                # multi-format-parser多格式解析器插件
  key_name log                # 在要解析的记录中指定字段名称
  reserve_data true           # 在解析结果中保留原始键值对
  remove_key_name_field true  # key_name 解析成功后删除字段。
  &lt;parse&gt;
    @type multi_format
    &lt;pattern&gt;
      format json
    &lt;/pattern&gt;
    &lt;pattern&gt;
      format none
    &lt;/pattern&gt;
  &lt;/parse&gt;
&lt;/filter&gt;
</code></pre>
<p>该过滤器使用 <code>json</code> 和 <code>none</code> 两个插件将 JSON 数据进行结构化，这样就会把 JSON 日志里面的属性解析成一个一个的字段，解析生效过后记得刷新 Kibana 的索引字段，否则会识别不了这些字段，通过 <code>管理</code> -&gt; <code>Index Pattern</code> 点击刷新字段列表即可。</p>
<p>下面我们将示例应用部署到 Kubernetes 集群中：(dummylogs.yaml)</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: dummylogs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dummylogs
  template:
    metadata:
      labels:
        app: dummylogs
        logging: &quot;true&quot;  # 要采集日志需要加上该标签
    spec:
      containers:
      - name: dummy
        image: cnych/dummylogs:latest
        args:
        - msg-processor
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dummylogs2
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dummylogs2
  template:
    metadata:
      labels:
        app: dummylogs2
        logging: &quot;true&quot;  # 要采集日志需要加上该标签
    spec:
      containers:
      - name: dummy
        image: cnych/dummylogs:latest
        args:
        - msg-receiver-api
</code></pre>
<p>直接部署上面的应用即可：</p>
<pre><code class="language-shell">$ kubectl apply -f dummylogs.yaml
$ kubectl get pods -l logging=true
NAME                         READY   STATUS    RESTARTS   AGE
counter                      1/1     Running   0          22h
dummylogs-6f7b56579d-7js8n   1/1     Running   5          15h
dummylogs-6f7b56579d-wdnc6   1/1     Running   5          15h
dummylogs-6f7b56579d-x4twn   1/1     Running   5          15h
dummylogs2-d9b978d9b-bchks   1/1     Running   5          15h
dummylogs2-d9b978d9b-wv7rj   1/1     Running   5          15h
dummylogs2-d9b978d9b-z2r26   1/1     Running   5          15h
</code></pre>
<p>部署完成后 dummylogs 和 dummylogs2 两个应用就会开始输出不同级别的日志信息了，记得要给应用所在的节点打上 <code>beta.kubernetes.io/fluentd-ds-ready=true</code> 的标签，否则 fluentd 不会在对应的节点上运行也就不会收集日志了。正常情况下日志就已经可以被采集到 Elasticsearch 当中了，我们可以前往 Kibana 的 Dashboard 页面查看:</p>
<figure data-type="image" tabindex="1"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428092342.png" alt="img" loading="lazy"></figure>
<p>我们可以看到可用的字段中已经包含我们应用中的一些字段了。找到 <code>serviceName</code> 字段点击我们可以查看已经采集了哪些服务的消息：</p>
<figure data-type="image" tabindex="2"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428092559.png" alt="img" loading="lazy"></figure>
<p>可以看到我们收到了来自 <code>msg-processor</code> 和 <code>msg-receiver-api</code> 的日志信息，在最近15分钟之内，<code>api</code> 服务产生的日志更多，点击后面的加号就可以只过滤该服务的日志数据：</p>
<figure data-type="image" tabindex="3"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428092903.png" alt="img" loading="lazy"></figure>
<p>我们可以看到展示的日志数据的属性比较多，有时候可能不利于我们查看日志，此时我们可以筛选想要展示的字段:</p>
<figure data-type="image" tabindex="4"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428093202.png" alt="img" loading="lazy"></figure>
<p>我们可以根据自己的需求选择要显示的字段，现在查看消息的时候就根据清楚了：</p>
<figure data-type="image" tabindex="5"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428093343.png" alt="img" loading="lazy"></figure>
<p>比如为了能够更加清晰的展示我们采集的日志数据，还可以将 <code>eventsNumber</code> 和 <code>serviceName</code> 字段选中添加：</p>
<figure data-type="image" tabindex="6"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428093646.png" alt="img" loading="lazy"></figure>
<p>然后同样我们可以根据自己的需求来筛选需要查看的日志数据：</p>
<figure data-type="image" tabindex="7"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428093815.png" alt="img" loading="lazy"></figure>
<p>如果你的 Elasticsearch 的查询语句比较熟悉的话，使用查询语句能实现的筛选功能更加强大，比如我们要查询 <code>mgs-processor</code> 和 <code>msg-receiver-api</code> 两个服务的日志，则可以使用如下所示的查询语句：</p>
<pre><code class="language-shell">serviceName:msg-processor OR serviceName:msg-receiver-api
</code></pre>
<p>直接搜索框中输入上面的查询语句进行查询即可：</p>
<figure data-type="image" tabindex="8"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428094158.png" alt="img" loading="lazy"></figure>
<p>接下来我们来创建一个图表来展示已经处理了多少 <code>msg-processor</code> 服务的日志信息。在 Kibana 中切换到 <code>Visualize</code> 页面，点击 <code>Create new visualization</code> 按钮选择 <code>Area</code>，选择 <code>k8s-*</code> 的索引，首先配置 Y 轴的数据，这里我们使用 <code>eventsNumber</code> 字段的 <code>Sum</code> 函数进行聚合：</p>
<figure data-type="image" tabindex="9"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428095222.png" alt="img" loading="lazy"></figure>
<p>然后配置 X 轴数据使用 <code>Date Histogram</code> 类型的 <code>@timestamp</code> 字段：</p>
<figure data-type="image" tabindex="10"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428095344.png" alt="img" loading="lazy"></figure>
<p>配置完成后点击右上角的 <code>Apply Changes</code> 按钮则就会在右侧展示出对应的图表信息：</p>
<figure data-type="image" tabindex="11"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428095631.png" alt="img" loading="lazy"></figure>
<p>这个图表展示的就是最近15分钟内被处理的事件总数，当然我们也可以自己选择时间范围。我们还可以将 <code>msg-receiver-api</code> 事件的数量和已处理的消息总数进行关联，在该图表上添加另外一层数据，在 Y 轴上添加一个新指标，选择 <code>Add metrics</code> 和 <code>Y-axis</code>，然后同样选择 <code>sum</code> 聚合器，使用 <code>volume</code> 字段：</p>
<figure data-type="image" tabindex="12"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428100341.png" alt="img" loading="lazy"></figure>
<p>点击 <code>Apply Changes</code> 按钮就可以同时显示两个服务事件的数据了。最后点击顶部的 <code>save</code> 来保存该图表，并为其添加一个名称。</p>
<p>在实际的应用中，我们可能对应用的错误日志更加关心，需要了解应用的运行情况，所以对于错误或者警告级别的日志进行统计也是非常有必要的。现在我们回到 <code>Discover</code> 页面，输入 <code>LOGLEVEL:ERROR OR LOGLEVEL:WARNING</code> 查询语句来过滤所有的错误和告警日志：</p>
<figure data-type="image" tabindex="13"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428101527.png" alt="img" loading="lazy"></figure>
<p>错误日志相对较少，实际上我们这里的示例应用会每 15-20 分钟左右就会抛出4个错误信息，其余都是警告信息。同样现在我们还是用可视化的图表来展示下错误日志的情况。同样切换到 <code>Visualize</code> 页面，点击 <code>Create visualization</code>，选择 <code>Vertical Bar</code>，然后选中 <code>k8s-*</code> 的 Index Pattern。</p>
<figure data-type="image" tabindex="14"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428102104.png" alt="img" loading="lazy"></figure>
<p>现在我们忽略 Y 轴，使用默认的 <code>Count</code> 设置来显示消息数量。首先点击 <code>Buckets</code> 下面的 <code>X-axis</code>，然后同样选择 <code>Date histogram</code>，然后点击下方的 <code>Add</code>，添加 <code>Sub-Bueckt</code>，选择 <code>Split series</code>:</p>
<figure data-type="image" tabindex="15"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428102530.png" alt="img" loading="lazy"></figure>
<p>然后我们可以通过指定的字段来分割条形图，选择 <code>Terms</code> 作为子聚合方式，然后选择 <code>serviceName.keyword</code> 字段，最后点击 <code>apply</code> 生成图表：</p>
<figure data-type="image" tabindex="16"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428102913.png" alt="img" loading="lazy"></figure>
<p>现在上面的图表以不同的颜色来显示每个服务消息，接下来我们在搜索框中输入要查找的内容，因为现在的图表是每个服务的所有消息计数，包括正常和错误的日志，我们要过滤告警和错误的日志，同样输入 <code>LOGLEVEL:ERROR OR LOGLEVEL:WARNING</code> 查询语句进行搜索即可：</p>
<figure data-type="image" tabindex="17"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428103237.png" alt="img" loading="lazy"></figure>
<p>从图表上可以看出来 <code>msg-processor</code> 服务问题较多，只有少量的是 <code>msg-receiver-api</code> 服务的，当然我们也可以只查看 <code>ERROR</code> 级别的日志统计信息：</p>
<figure data-type="image" tabindex="18"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428103446.png" alt="img" loading="lazy"></figure>
<p>从图表上可以看出来基本上出现错误日志的情况下两个服务都会出现，所以这个时候我们就可以猜测两个服务的错误是非常相关的了，这对于我们去排查错误非常有帮助。最后也将该图表进行保存。</p>
<p>最后我们也可以将上面的两个图表添加到 <code>dashboard</code> 中，这样我们就可以在一个页面上组合各种可视化图表。切换到 <code>dashboard</code> 页面，然后点击 <code>Create New Dashboard</code> 按钮：</p>
<figure data-type="image" tabindex="19"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428104152.png" alt="img" loading="lazy"></figure>
<p>选择 <code>Add an existing</code> 链接：</p>
<figure data-type="image" tabindex="20"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428104225.png" alt="img" loading="lazy"></figure>
<p>然后选择上面我们创建的两个图表，添加完成后同样保存该 <code>dashboard</code> 即可：</p>
<figure data-type="image" tabindex="21"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428104516.png" alt="img" loading="lazy"></figure>
<p>到这里我们就完成了通过 Fluentd 收集日志到 Elasticsearch，并通过 Kibana 对日志进行了分析可视化操作。</p>
<h2 id="基于日志的报警">基于日志的报警</h2>
<p>在生产环境中我们往往都会使用 Promethus 对应用的各项指标进行监控，但是往往应用的日志中也会产生一些错误日志，这些信息并不是都能够通过 metrics 提供数据的，所以为了避免出现太多的错误，我们还需要对错误日志进行监控报警。在 Elasticsearch 中，我们可以通过使用 <code>elastalert</code> 组件来完成这个工作。</p>
<p><a href="https://github.com/Yelp/elastalert">elastalert</a> 是 yelp 使用 python 开发的 elasticsearch 告警工具。<code>elastalert</code> 依照一定频率查询 ES，将查询结果对比告警阈值，超过阈值即进行告警。告警方式包括但不局限于邮箱、微信、钉钉等。</p>
<p>我们这里将 <code>elastalert</code> 部署到 Kubernetes 集群中，对应的资源清单文件如下所示：(elastalert.yaml)</p>
<pre><code class="language-yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: elastalert-config
  namespace: logging
  labels:
    app: elastalert
data:
  elastalert_config: |-
    ---
    rules_folder: /opt/rules       # 指定规则的目录
    scan_subdirectories: false
    run_every:                     # 多久从 ES 中查询一次
      minutes: 1
    buffer_time:
      minutes: 15
    es_host: elasticsearch
    es_port: 9200
    writeback_index: elastalert
    use_ssl: False
    verify_certs: True
    alert_time_limit:             # 失败重试限制
      minutes: 2880
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: elastalert-rules
  namespace: logging
  labels:
    app: elastalert
data:
  rule_config.yaml: |-
    name: dummylogs error     # 规则名字，唯一值
    es_host: elasticsearch
    es_port: 9200
    
    type: any                 # 报警类型
    index: k8s-*              # es索引
    
    filter:                   # 过滤
    - query:
        query_string:
          query: &quot;LOGLEVEL:ERROR&quot;  # 报警条件

    alert:                    # 报警类型
    - &quot;email&quot;
    smtp_host: smtp.qq.com
    smtp_port: 587
    smtp_auth_file: /opt/auth/smtp_auth_file.yaml
    email_reply_to: 517554016@qq.com
    from_addr: 517554016@qq.com
    email:                  # 接受邮箱
    - &quot;ych_1024@163.com&quot;
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elastalert
  namespace: logging
  labels:
    app: elastalert
spec:
  selector:
    matchLabels:
      app: elastalert
  template:
    metadata:
      labels:
        app: elastalert
    spec:
      containers:
      - name: elastalert
        image: jertel/elastalert-docker:0.2.4
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: config
          mountPath: /opt/config
        - name: rules
          mountPath: /opt/rules
        - name: auth
          mountPath: /opt/auth
        resources:
          limits:
            cpu: 50m
            memory: 256Mi
          requests:
            cpu: 50m
            memory: 256Mi
      volumes:
      - name: auth
        secret:
          secretName: smtp-auth
      - name: rules
        configMap:
          name: elastalert-rules
      - name: config
        configMap:
          name: elastalert-config
          items:
          - key: elastalert_config
            path: elastalert_config.yaml
</code></pre>
<p>使用邮件进行报警的时候，需要指定一个 <code>smtp_auth_file</code> 的文件，文件中包含用户名和密码：(smtp_auth_file.yaml)</p>
<pre><code class="language-yaml">user: &quot;xxxxx@qq.com&quot;       # 发送的邮箱地址
password: &quot;ewwghfhdvjwnbjea&quot;   # 不是qq邮箱的登录密码，是授权码
</code></pre>
<p>然后使用上面的文件创建一个对应的 Secret 资源对象：</p>
<pre><code class="language-shell">$ kubectl create secret generic smtp-auth --from-file=smtp_auth_file.yaml -n logging
</code></pre>
<p>然后直接创建上面的 elastalert 应用：</p>
<pre><code class="language-shell">$ kubectl apply -f elastalert.yaml
$ kubectl get pods -n logging -l app=elastalert
NAME                          READY   STATUS    RESTARTS   AGE
elastalert-64ccfbffcf-gd6xz   1/1     Running   0          102s
$ kubectl logs -f elastalert-64ccfbffcf-gd6xz -n logging
Elastic Version: 7.6.2
Reading Elastic 6 index mappings:
Reading index mapping 'es_mappings/6/silence.json'
Reading index mapping 'es_mappings/6/elastalert_status.json'
Reading index mapping 'es_mappings/6/elastalert.json'
Reading index mapping 'es_mappings/6/past_elastalert.json'
Reading index mapping 'es_mappings/6/elastalert_error.json'
Deleting index elastalert_status.
New index elastalert created
Done!
</code></pre>
<p>看到上面的日志信息就证明 elastalert 应用部署成功了。在 Elasticsearch 中也可以看到几个相关的 Index ：</p>
<figure data-type="image" tabindex="22"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428121319.png" alt="img" loading="lazy"></figure>
<p>由于我们的示例应用会隔一段时间就产生 ERROR 级别的错误日志，所以正常情况下我们就可以收到如下所示的邮件信息了：</p>
<figure data-type="image" tabindex="23"><img src="https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20200428121055.png" alt="img" loading="lazy"></figure>
<p>除此之外我们也可以配置将报警信息发往 <a href="https://github.com/anjia0532/elastalert-wechat-plugin">企业微信</a> 或者 <a href="https://github.com/xuyaoqiang/elastalert-dingtalk-plugin">钉钉</a>，还可以安装一个 elastalert 的 <a href="https://github.com/bitsensor/elastalert-kibana-plugin">Kibana 插件</a>，用于在 Kibana 页面上进行可视化操作。</p>
<p>关于 elastalert 更多的操作和使用说明，大家可以查看官方文档了解更多：https://elastalert.readthedocs.io/en/latest/。</p>
<p>原文链接 https://www.qikqiak.com/post/install-efk-stack-on-k8s/</p>

          
          <p class="next-post">下一篇：
            <a href="https://jwangkun.github.io/J3AaqcxFd/">
              <span class="post-title">
                win10上搭建k8s集群&rarr;
              </span>
            </a>
          </p>
        
        <div class="comment">
          
        </div>
      </div>
    </div>
  </article>
 <!-- Footer -->
  <footer>
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
          <ul class="list-inline text-center">
            
            
              
            
              
            
              
            
              
            
              
            
              
            
              
              <li class="list-inline-item">
              <a href="https://jwangkun.github.io/atom.xml" target="_blank">
                <span class="fa-stack fa-lg">
                  <i class="fas fa-circle fa-stack-2x"></i>
                  <i class="fas fa-rss fa-stack-1x fa-inverse"></i>
                </span>
              </a>
              </li>
          </ul>
          <p class="copyright text-muted">Copyright &copy;<span>John Wong&#39;s Blog</span><br><a href="https://github.com/getgridea/gridea" class="Themeinfo">Powered by Gridea</a></p>
        </div>
      </div>
    </div>
   </footer>
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.bootcss.com/twitter-bootstrap/4.3.1/js/bootstrap.bundle.min.js"></script>
  <!-- <script src="https://jwangkun.github.io/media/scripts/bootstrap.bundle.min.js"></script> -->
  <!-- Bootstrap core JavaScript -->
  <script src="https://cdn.jsdelivr.net/gh/Alanrk/clean-cdn@1.0/scripts/clean-blog.min.js"></script>
  <!-- <script src="https://jwangkun.github.io/media/scripts/clean-blog.min.js"></script> -->
  <script src="//instant.page/3.0.0" type="module" defer integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1"></script>
  <style type="text/css">a.back_to_top{text-decoration:none;position:fixed;bottom:40px;right:30px;background:#f0f0f0;height:40px;width:40px;border-radius:50%;line-height:36px;font-size:18px;text-align:center;transition-duration:.5s;transition-propety:background-color;display:none}a.back_to_top span{color:#888}a.back_to_top:hover{cursor:pointer;background:#dfdfdf}a.back_to_top:hover span{color:#555}@media print,screen and(max-width:580px){.back_to_top{display:none!important}}</style>
<a id="back_to_top" href="#" class="back_to_top">
  <span>▲</span></a>
<script>$(document).ready((function(_this) {
    return function() {
      var bt;
      bt = $('#back_to_top');
      if ($(document).width() > 480) {
        $(window).scroll(function() {
          var st;
          st = $(window).scrollTop();
          if (st > 30) {
            return bt.css('display', 'block')
          } else {
            return bt.css('display', 'none')
          }
        });
        return bt.click(function() {
          $('body,html').animate({
            scrollTop: 0
          },
          800);
          return false
        })
      }
    }
  })(this));
  var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?84ab85460bfbe79dbe5776a1df139a8f";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
  </script>
  
<script type="text/javascript" src="https://v1.cnzz.com/z_stat.php?id=1279350888&web_id=1279350888"></script>

  </body>
</html>

